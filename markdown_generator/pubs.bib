

@article{ref:Rakitianskaia:2012,
	title={Training feedforward neural networks with dynamic particle swarm optimisation},
	author={Rakitianskaia, Anna S and Engelbrecht, Andries Petrus},
	journal={Swarm Intelligence},
	volume={6},
	number={3},
	pages={233--270},
	year={2012},
	publisher={Springer}
}

@article{ref:Bosman:2017,
	title={Fitness Landscape Analysis of Weight-Elimination Neural Networks},
	author={Bosman, Anna Sergeevna and Engelbrecht, Andries P and Helbig, Mard{\'e}},
	journal={Neural Processing Letters},
	volume={48},
	number={1},
	pages={353--373},
	year={2018},
	publisher={Springer}
}

@phdthesis{ref:Bosman:PhD:2019,
	author = {Bosman, Anna Sergeevna},
	title = {Fitness Landscape Analysis of Feed-Forward Neural Networks},
	school = {University of Pretoria},
	year = {2019},
	url = {https://repository.up.ac.za/handle/2263/70634},
	abstract = {Neural network training is a highly non-convex optimisation problem with poorly understood properties. Due to the inherent high dimensionality, neural network search spaces cannot be intuitively visualised, thus other means to establish search space properties have to be employed. Fitness landscape analysis encompasses a selection of techniques designed to estimate the properties of a search landscape associated with an optimisation problem. Applied to neural network training, fitness landscape analysis can be used to establish a link between the properties of the error landscape and various neural network hyperparameters. This study applies fitness landscape analysis to investigate the influence of the search space boundaries, regularisation parameters, loss functions, activation functions, and feed-forward neural network architectures on the properties of the resulting error landscape. A novel gradient-based sampling technique is proposed, together with a novel method to quantify and visualise stationary points and the associated basins of attraction in neural network error landscapes.},
	keywords = {neural networks, fitness landscape analysis, classification, visualisation, loss surfaces, modality}
}

@article{bosman2020visualising,
  title={Visualising basins of attraction for the cross-entropy and the squared error neural network loss functions},
  author={Bosman, Anna Sergeevna and Engelbrecht, Andries and Helbig, Mard{\'e}},
  journal={Neurocomputing},
  volume={400},
  pages={113--136},
  year={2020},
  publisher={Elsevier}
}

@article{ellis2021characterisation,
  title={Characterisation of environment type and difficulty for streamed data classification problems},
  author={Ellis, Mathys and Bosman, Anna S and Engelbrecht, Andries P},
  journal={Information Sciences},
  volume={569},
  pages={615--649},
  year={2021},
  publisher={Elsevier}
}

@article{van2022kasam,
  title={KASAM: Spline Additive Models for Function Approximation},
  author={van Deventer, Heinrich and van Rensburg, Pieter Janse and Bosman, Anna},
  journal={arXiv preprint arXiv:2205.06376},
  year={2022}
}

@article{van2022atlas,
  title={ATLAS: Universal Function Approximator for Memory Retention},
  author={van Deventer, Heinrich and Bosman, Anna},
  journal={arXiv preprint arXiv:2208.05388},
  year={2022}
}

@article{panagiotou2023denoising,
  title={Denoising Diffusion Post-Processing for Low-Light Image Enhancement},
  author={Panagiotou, Savvas and Bosman, Anna S},
  journal={arXiv preprint arXiv:2303.09627},
  year={2023}
}

@article{schreuder2023training,
  title={Training Feedforward Neural Networks with Bayesian Hyper-Heuristics},
  author={Schreuder, Arn{\'e} and Bosman, Anna and Engelbrecht, Andries and Cleghorn, Christopher},
  journal={arXiv preprint arXiv:2303.16912},
  year={2023}
}

